# spark-file-analyzer
Tout le monde pense maÃ®triser les logs... jusqu'Ã  ce dÃ©bug nocturne en prod Ã  3h du matin, 6 mois aprÃ¨s avoir touchÃ© au code pour la derniÃ¨re fois. ðŸŒ™

5 tips essentiels pour des logs qui vous sauveront la vie :

1ï¸âƒ£ DÃ¨s que possible, Adoptez le logging structurÃ© : timestamp, error_type, user_id... Fini le parsing de texte Ã  3h du mat' !

2ï¸âƒ£ Utilisez les niveaux de log intelligemment 
 ðŸŸ© INFO : "Commande validÃ©e" 
 ðŸŸ¨ WARN : "Paiement ralenti"
 ðŸŸ¥ ERROR : "Connexion DB perdue" 
 âŒ FATAL : "Plus de mÃ©moire, arrÃªt systÃ¨me"

3ï¸âƒ£ Contextualisez TOUJOURS vos logs 
WHO : ID utilisateur 
WHAT : Action rÃ©alisÃ©e 
WHERE : Service concernÃ© 
WHY : Description dÃ©taillÃ©e

4ï¸âƒ£ Une ligne de log = Une histoire complÃ¨te Capturez tout le processus en une seule entrÃ©e : action, rÃ©sultat, durÃ©e. Votre "vous du futur" vous remerciera.

5ï¸âƒ£ Bannissez les donnÃ©es sensibles Pas de mot de passe, carte bancaire ou clÃ© d'API dans les logs. Jamais. Jamais ... Jamais

## Exemple de workflow dans un projet plus large :
1. **CrÃ©er une nouvelle branche pour chaque tÃ¢che** :
   ```bash
   git checkout -b feature/nouvelle-fonctionnalite
   ```
2. **Committer les changements rÃ©guliÃ¨rement** :

   ```bash
   git add .
   git commit -m "Ajout de la fonctionnalitÃ© X"
   ```
3. **Mettre Ã  jour ta branche main avant de fusionner** :

   ```bash
   git checkout main
   git pull origin main
   ```

4. **Fusionner ta branche dans main** :

   ```bash
   git merge feature/nouvelle-fonctionnalite
   ```

5. **RÃ©soudre les conflits (si nÃ©cessaire) et pousser sur GitHub** :

   ```bash
    git push origin main
   ```

6. **Supprimer les branches locales et distantes aprÃ¨s fusion** :

   ```bash
    git branch -d feature/nouvelle-fonctionnalite
    git push origin --delete feature/nouvelle-fonctionnalite
   ```

# Understand Spark Workflow:

Write Scala code â†’ Compile (sbt) â†’ Package â†’ Run (spark-submit).

Understand spark-submit spark-submit is the tool used to run Spark applications on a cluster or a standalone environment. It handles:

Deploying your JAR file. Configuring Spark runtime options (e.g., memory, cores, environment variables). Running your application.

# a sample exemple with code:

1. Write the Scala Code
Write your Spark application in Scala. For example, a program that reads data and performs some transformations:
   ```scala
import org.apache.spark.sql.SparkSession

object SparkFileAnalyzer {
  def main(args: Array[String]): Unit = {
    // Step 1: Create SparkSession
    val spark = SparkSession.builder()
      .appName("File Analyzer")
      .master("local[*]") // Use all cores locally
      .getOrCreate()

    // Step 2: Read data
    val inputPath = args(0) // Pass input path as argument
    val data = spark.read.text(inputPath)

    // Step 3: Process data
    val wordCounts = data
      .flatMap(row => row.getString(0).split(" ")) // Split lines into words
      .groupBy("value") // Group by word
      .count()          // Count occurrences

    // Step 4: Write result
    val outputPath = args(1) // Pass output path as argument
    wordCounts.write.csv(outputPath)

    // Stop SparkSession
    spark.stop()
  }
}
   ```
2. Compile and Build (sbt)
Create an sbt project structure:
   ```css
project/
    build.properties
src/
    main/
        scala/
            SparkFileAnalyzer.scala
build.sbt
   ```
Write build.sbt:
   ```sbt
name := "sparkfileanalyzer"

version := "0.1"

scalaVersion := "2.12.15"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.3.2",
  "org.apache.spark" %% "spark-sql" % "3.3.2"
)
   ```
Compile and package:
Run the following commands in the project directory:

   ```bash
sbt clean compile
sbt package
   ```
This generates a JAR file (e.g., target/scala-2.12/sparkfileanalyzer_2.12-0.1.jar).

3. Run the Application (spark-submit)
Run your JAR file using spark-submit. For example:

   ```bash
/opt/spark/bin/spark-submit \
    --class SparkFileAnalyzer \
    --master local[*] \
    /path/to/target/scala-2.12/sparkfileanalyzer_2.12-0.1.jar \
    /path/to/input.txt /path/to/output
       ```
What Happens Here:
--class SparkFileAnalyzer: Specifies the main class to execute.
--master local[*]: Runs locally using all available CPU cores.
JAR Path: Deploys your compiled code (JAR) to Spark.
Arguments: /path/to/input.txt and /path/to/output are passed to your application as arguments.
4. What Happens During Execution
Step 1: Spark initializes a session using your configuration (local[*] here).
Step 2: Spark reads the input data (e.g., a text file).
Step 3: The data is processed (split into words, counted, etc.).
Step 4: The results are saved to the output directory.
For example:

Input (input.txt):
   ```
hello world
hello Spark
```
Output (CSV files in /path/to/output):

```
hello, 2
world, 1
Spark, 1
```
